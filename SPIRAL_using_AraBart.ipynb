{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OALYZuHXbnMr",
        "outputId": "09e3368d-c802-479f-a3ed-34dfb5634896"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU-0ijRL_IwD",
        "outputId": "41f08932-b677-46a8-f0b7-d756208eb263"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Tue Jun 28 20:41:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MOsHUjgdIrIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8330930d-8a39-46b1-cf56-960000d208e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 32.4 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 69.6 MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 13.4 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 71.8 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 102.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 64.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 95.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Installing collected packages: urllib3, multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, huggingface-hub, fsspec, aiohttp, xxhash, transformers, sentencepiece, responses, portalocker, colorama, sacrebleu, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 colorama-0.4.5 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 portalocker-2.4.0 pyyaml-6.0 responses-0.18.0 sacrebleu-2.1.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.20.1 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers[sentencepiece] sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vtK2UtEKY_o3",
        "outputId": "9ac45098-e1bb-4e5b-e352-438bf98e5daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.20.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a normalization task\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nuT6I68IY_o6"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"moussaKam/AraBART\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HQ_E6OUb2k7",
        "outputId": "c2de6b15-5805-4ec9-de57-f6a6bcdd1bca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 44 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=5723b1b3612a76108031802494ce6cb46a20db0d4c0cdeca9bda6b420df5b3b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "3f924eca49b548dbbc407f34d141eb1a",
            "b1123bcbe1a947a3a1859dc9091a602b",
            "0ca046404858491b963f18b1412e406e",
            "815f815399c34262ad5fd8a7fa00fca9",
            "0fbd859033104837929e81833c1d841b",
            "0213cad7078f484999367aa60ecb43f9",
            "5a26fca2d84743e39577d67350e6fea8",
            "85b235646e7a4b2a816b95f437b1e712",
            "24c7c776f9154502afd6b3f4ce164613",
            "f39196abbfa8479d9aacedc287b7caea",
            "9becee0627ee4d7b8199512fc2d0a20a",
            "9c91bfc5064e452d962e90dbad12c18a",
            "5a78a840ca934459bdc14d491d5aec20",
            "ce0c226bb8244176a78cd98cfea6afe2",
            "2caef253d9b0433290da2bdcce24db4e",
            "0537aca5b6574a359d6a5e6583080b8d",
            "38dc8aa992444841be12459b86ee70dd",
            "433fb79895e6455994154f08d0f923d7",
            "a5cdbf61fa324d87a09bd48919586025",
            "769376e3127442959351b6dcf86e3d21",
            "da2364e7ac3b47759892f33e54964de8",
            "aab4504aa5174624bfd6afd43cb55405",
            "2ce541cbf6b946e2b25a54e2438257a1",
            "e0c285a66e444d38911e15c5bc4ecef0",
            "b78f8a9e7b65449495d56a592e5a19e6",
            "cbe3186924d24e989c1d8a3c91e9ee47",
            "cb7e2a953b624620afac3b82c03c2d12",
            "dcf6976eff6b49afbd8c108819b83173",
            "c1b8ae7b88c74633b83b246bd9cfcf17",
            "c40e99ace384424db5aa43d9642acc5a",
            "caaba9446a2d474b93e5de66d564cbba",
            "3318ffa4e53c4a218bd0fa9b85d50614",
            "1f7de103b1084e52a141df35c5beae50",
            "6953a6dc78e2430eba14e9bbf05b0c7b",
            "5815ce8fb8544a3a97b9e93b1876a199",
            "6ada4f3ec9bc4278b4e8e0ce221097c1",
            "54888541df524bc68baf6f38dd60b8cf",
            "a42e80ee6dca4672b1b3f120508bc5b4",
            "05d027ee85594c22b70ab132cfd403b0",
            "bf0bdcd1e3d94576b2bb4504e314b86b",
            "45dd3d60631e48bc9f6548e9ca42728f",
            "f988ca6e4ae54b07a1203c8e8bb06abc",
            "5bf7c4085f4d46a3aac22ef99f90d184",
            "b8d7fbc83cf9494291fe08df06bcb6f4",
            "e3f3c0629e724047b5982f82d9bb6ff4",
            "d4a990cc7035495d9893cb4fe0f8b186",
            "a1cd1d6f81f94a529eedead7552facb1",
            "798128f807c54452b1b0e6857c7f8faf",
            "f4419c64d9ef42108031815e2fbc9b65",
            "f7d5ab2c00c54b1d982b63245ddc47de",
            "45be3ec808a74021b7c7df26189a4fac",
            "04e95d491ce94b4da5fbb24f8e739b39",
            "8cf5bef130ad4fc4b66c525d48dc9ca6",
            "516c8eb658a649a28180dc187d24b1a2",
            "369fc48afcbb416587377793ddaddf9e",
            "44a9b2ffb744407587e49a795646c3d4",
            "a6d6719b993d4debbbb5537e8c2b252e",
            "a7acf550fb03470d8679a473d037ac2c",
            "2beef0a8b8954d329ee88ae737a4d7bc",
            "3cddd5e2b8f34993a77f42f6be81f220",
            "74b2d54608574c1db8a70e0668f88e50",
            "7da4f8dc151b467a84d67d13d60b84a9",
            "3eeff5dd26b44482bb714ebe7128f998",
            "a29342bcdf3d46969085fafb7f3446a6",
            "99ef2d19f8cd4139a2359f81ba80e8cc",
            "c4c606adbd3643399db2aa13abb40a63"
          ]
        },
        "outputId": "64ccbb23-bf2f-4e0f-a303-d8584c1b25ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-6fdb8b7f00d26e28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-6fdb8b7f00d26e28/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f924eca49b548dbbc407f34d141eb1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c91bfc5064e452d962e90dbad12c18a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0 tables [00:00, ? tables/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ce541cbf6b946e2b25a54e2438257a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0 tables [00:00, ? tables/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6953a6dc78e2430eba14e9bbf05b0c7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-6fdb8b7f00d26e28/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3f3c0629e724047b5982f82d9bb6ff4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44a9b2ffb744407587e49a795646c3d4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "raw_datasets = load_dataset('csv', data_files={'train': ['/content/drive/MyDrive/Academic_Papers/SPIRAL/Space_issues/train.csv'],'test': '/content/drive/MyDrive/Academic_Papers/SPIRAL/Space_issues/test.csv'})\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GWiVUF0jIrIv",
        "outputId": "2fcba685-8325-4657-ee7b-d314e2b34f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['index', 'word', 'Norm'],\n",
              "        num_rows: 1716190\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['index', 'word', 'Norm'],\n",
              "        num_rows: 429047\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK1khGG_nFz9",
        "outputId": "3c7eb034-191c-462b-acf1-c60cf5aa5189"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
              "Produces BLEU scores along with its sufficient statistics\n",
              "from a source against one or more references.\n",
              "\n",
              "Args:\n",
              "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
              "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
              "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
              "        - `'none'`: no smoothing\n",
              "        - `'floor'`: increment zero counts\n",
              "        - `'add-k'`: increment num/denom by k for n>1\n",
              "        - `'exp'`: exponential decay\n",
              "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
              "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
              "        - `'none'`: No tokenization.\n",
              "        - `'zh'`: Chinese tokenization.\n",
              "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
              "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
              "        - `'char'`: Language-agnostic character-level tokenization.\n",
              "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
              "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
              "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
              "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
              "\n",
              "Returns:\n",
              "    'score': BLEU score,\n",
              "    'counts': Counts,\n",
              "    'totals': Totals,\n",
              "    'precisions': Precisions,\n",
              "    'bp': Brevity penalty,\n",
              "    'sys_len': predictions length,\n",
              "    'ref_len': reference length,\n",
              "\n",
              "Examples:\n",
              "\n",
              "    Example 1:\n",
              "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
              "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
              "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
              "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
              "        >>> print(list(results.keys()))\n",
              "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
              "        >>> print(round(results[\"score\"], 1))\n",
              "        100.0\n",
              "\n",
              "    Example 2:\n",
              "        >>> predictions = [\"hello there general kenobi\",\n",
              "        ...                 \"on our way to ankh morpork\"]\n",
              "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
              "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
              "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
              "        >>> results = sacrebleu.compute(predictions=predictions,\n",
              "        ...                             references=references)\n",
              "        >>> print(list(results.keys()))\n",
              "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
              "        >>> print(round(results[\"score\"], 1))\n",
              "        39.8\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "f7c8b1a5-ab7d-4274-dfcc-330d5b444a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Norm': 'بمزاولة', 'index': 63220, 'word': 'ب مزاولة'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "raw_datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "outputId": "a7209eb8-464d-427a-b3b1-b7bf7a133bec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>word</th>\n",
              "      <th>Norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82034</td>\n",
              "      <td>والم رتدّون</td>\n",
              "      <td>والمرتدّون</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42491</td>\n",
              "      <td>أد اؤه</td>\n",
              "      <td>أداؤه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>319467</td>\n",
              "      <td>نظر تنّ</td>\n",
              "      <td>نظرتنّ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>74410</td>\n",
              "      <td>أ نتنت</td>\n",
              "      <td>أنتنت</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>202454</td>\n",
              "      <td>أ ضا</td>\n",
              "      <td>أضا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_random_elements(raw_datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCNhuTfGoylL",
        "outputId": "0c75f05f-4609-4f26-ea27-1d2e1e33e304"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
              "Produces BLEU scores along with its sufficient statistics\n",
              "from a source against one or more references.\n",
              "\n",
              "Args:\n",
              "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
              "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
              "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
              "        - `'none'`: no smoothing\n",
              "        - `'floor'`: increment zero counts\n",
              "        - `'add-k'`: increment num/denom by k for n>1\n",
              "        - `'exp'`: exponential decay\n",
              "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
              "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
              "        - `'none'`: No tokenization.\n",
              "        - `'zh'`: Chinese tokenization.\n",
              "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
              "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
              "        - `'char'`: Language-agnostic character-level tokenization.\n",
              "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
              "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
              "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
              "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
              "\n",
              "Returns:\n",
              "    'score': BLEU score,\n",
              "    'counts': Counts,\n",
              "    'totals': Totals,\n",
              "    'precisions': Precisions,\n",
              "    'bp': Brevity penalty,\n",
              "    'sys_len': predictions length,\n",
              "    'ref_len': reference length,\n",
              "\n",
              "Examples:\n",
              "\n",
              "    Example 1:\n",
              "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
              "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
              "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
              "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
              "        >>> print(list(results.keys()))\n",
              "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
              "        >>> print(round(results[\"score\"], 1))\n",
              "        100.0\n",
              "\n",
              "    Example 2:\n",
              "        >>> predictions = [\"hello there general kenobi\",\n",
              "        ...                 \"on our way to ankh morpork\"]\n",
              "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
              "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
              "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
              "        >>> results = sacrebleu.compute(predictions=predictions,\n",
              "        ...                             references=references)\n",
              "        >>> print(list(results.keys()))\n",
              "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
              "        >>> print(round(results[\"score\"], 1))\n",
              "        39.8\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eXNLu_-nIrJI",
        "outputId": "be335853-2ecd-438a-a4cc-fedd4225f22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "6f367d2a50d34312b1ce023f10b6904e",
            "09cd0bbb17a7469caf70fce46f1c0fee",
            "e028932bd7424cffb3c00a46ddfb0299",
            "a9a04715d0db4a29a2b9223e9f473928",
            "7d3415a72e4144e9a6fdcfcaf4fca5d4",
            "636b7dd8d8b249abb581b16543e35bba",
            "e9f7c65f206f4c9d975ba68b48543865",
            "ed05134573c0443494aafd5b21c69d28",
            "9abf7b92b623460c949796d207527472",
            "03a1b39ca9224e37a021777ee0cee5ef",
            "d6b93a0743334897acd0efe98c50532e",
            "1ec1397013594652b81962d443518503",
            "61106adbd75b41e0a789944c63e9be26",
            "f16e37a798c34adcaedc515187c085d0",
            "093932d6083f415787f68061ec1b4530",
            "9dff117a40244d98b4b2e58a6cee3fea",
            "8ac40a0ea18c429982bde6e560413669",
            "20ed39460100414ba2c215004530a535",
            "21b54d5812734831aa855ea2db7daa29",
            "2de39ce7144c4b21babe492379ec9d25",
            "253e1d153ad547118dd141d9b606dd39",
            "b265bdce69d94b46afbd32f13929c9e6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f367d2a50d34312b1ce023f10b6904e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ec1397013594652b81962d443518503"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "36hE1YacY_pE"
      },
      "outputs": [],
      "source": [
        "prefix = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 16\n",
        "max_target_length = 16\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"word\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"Norm\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "df342448-52bd-4806-ec21-0282c6445d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 35, 26554, 2], [0, 25214, 502, 112, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1]], 'labels': [[0, 690, 34817, 13, 2], [0, 25214, 1399, 2]]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "bc4bca72ffab4b2d80fa132ab4045e90",
            "1b941cc09c7e41e3ac5e4affcabd85e5",
            "3df67841aeab49b7933b6a03b4100707",
            "031b2eb51cf14e0eba3d1c2522b2bd8c",
            "f374a6338af949a488e66a3acfee74b9",
            "4418b47dff764c67a3efbb8bb2b95f18",
            "db38d17ee9a94aa5ab645470a88de629",
            "adbfdf5725f84e9190d64090d60d7cd0",
            "d7c18e78c6914d65b30a8eba8256c0d6",
            "47c343c2abcc47e5b23cffa03bda56d4",
            "37742003dab84ecfaf22cac2e0e3fdba",
            "ae60d8e07c444ffeb174a25e8b1bbdb9",
            "2a5e9e9966734468883c4f356fa7fb82",
            "976dbbe7783a4bcdb7d6b44921b776d7",
            "94c4dc2ba5904abb90fee1a9de8138e4",
            "af6942188b3047d28845b13d8c5979af",
            "bf63cd35576b49a19e1ec557c0fe8b2d",
            "0b321c55ba90496ab1d0d69efc3de969",
            "18cc64d6def74040b473fe790354d7a8",
            "fd198a4066e64333a4505b9691ddd4d6",
            "1b019a6d034749188af374b63666e650",
            "bd66d23dacdb4c6ca595e2858524ea6e"
          ]
        },
        "outputId": "001b883d-05dc-4157-9663-14f1bb163b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parameter 'function'=<function preprocess_function at 0x7ff9c483e170> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1717 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc4bca72ffab4b2d80fa132ab4045e90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/430 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae60d8e07c444ffeb174a25e8b1bbdb9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1f2f04c1009c497c840566bc1bda0f38",
            "e8f889d1eef24019ae7f2e2811e3f139",
            "99a7bc0f0fba4be9a1e3192c7f90ff82",
            "634d5bb4cfd64ba59d4b02b81e6e6114",
            "e248f3c7ac9941be91c748f517858d4d",
            "5883db857d0c46ffbbc652c4d4797475",
            "af245c9882ef47408748109a5888fcb7",
            "6e44d038eb3846b39322b95d33405a40",
            "6554bfce57244512acfeb9b2c7f5d0c8",
            "2995a1b87f6942bd87889fb64b946996",
            "744dd9c6ca4443ebbc66d6256795bd78"
          ]
        },
        "outputId": "1f40f959-2a6d-4f0d-ecd5-d5333b537ce5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/531M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f2f04c1009c497c840566bc1bda0f38"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang='word'\n",
        "target_lang='norm'"
      ],
      "metadata": {
        "id": "Q4j2IAtncjQq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=15,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vU_UlV1QY_pH"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "\n",
        "    result1= accuracy_score(decoded_labels, decoded_preds)\n",
        "    results2= f1_score(decoded_labels, decoded_preds, average='macro')\n",
        "    results3= precision_recall_fscore_support(decoded_labels, decoded_preds, average='macro')\n",
        "    \n",
        "    return {\n",
        "        \"f1-score\": results2,\n",
        "        \"accuracy\": result1,\n",
        "        \"All\": results3,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "imY1oC3SIrJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc626b72-a47a-4587-8343-f751237841a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b573928-651c-4bd9-c4d6-4cc576b45a71"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1716190\n",
            "  Num Epochs = 15\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 402240\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26817' max='402240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 26817/402240 1:06:14 < 15:27:25, 6.75 it/s, Epoch 1/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2370' max='6704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2370/6704 16:18 < 29:49, 2.42 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-500/special_tokens_map.json\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-1000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-1000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-1500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-1500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-1500/special_tokens_map.json\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-2000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-2000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-2000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-2500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-2500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-2500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-1000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-3000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-3000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-3000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-1500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-3500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-3500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-3500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-2000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-4000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-4000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-4000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-2500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-4500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-4500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-4500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-3000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-5000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-5000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-5000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-3500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-5500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-5500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-5500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-4000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-6000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-6000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-6000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-4500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-6500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-6500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-6500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-5000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-7000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-7000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-7000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-5500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-7500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-7500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-7500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-6000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-8000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-8000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-8000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-6500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-8500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-8500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-8500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-7000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-9000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-9000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-9000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-7500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-9500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-9500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-9500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-8000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-10000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-10000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-10000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-8500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-10500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-10500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-10500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-9000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-11000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-11000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-11000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-9500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-11500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-11500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-11500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-10000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-12000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-12000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-12000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-10500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-12500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-12500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-12500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-11000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-13000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-13000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-13000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-11500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-13500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-13500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-13500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-12000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-14000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-14000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-14000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-12500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-14500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-14500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-14500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-13000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-15000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-15000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-15000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-13500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-15500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-15500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-15500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-14000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-16000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-16000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-16000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-14500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-16500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-16500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-16500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-15000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-17000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-17000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-17000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-15500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-17500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-17500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-17500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-16000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-18000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-18000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-18000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-16500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-18500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-18500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-18500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-17000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-19000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-19000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-19000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-17500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-19500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-19500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-19500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-18000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-20000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-20000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-20000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-18500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-20500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-20500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-20500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-19000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-21000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-21000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-21000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-19500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-21500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-21500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-21500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-20000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-22000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-22000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-22000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-20500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-22500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-22500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-22500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-21000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-23000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-23000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-23000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-21500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-23500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-23500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-23500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-22000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-24000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-24000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-24000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-22500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-24500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-24500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-24500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-23000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-25000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-25000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-25000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-23500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-25500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-25500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-25500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-24000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-26000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-26000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-26000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-24500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-26500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-26500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-26500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-25000] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 429047\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160897' max='402240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160897/402240 10:36:24 < 15:54:36, 4.21 it/s, Epoch 6/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1-score</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>All</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.158945</td>\n",
              "      <td>0.790217</td>\n",
              "      <td>0.879955</td>\n",
              "      <td>(0.8048262400993602, 0.7932012922999004, 0.7902166867298706, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.135200</td>\n",
              "      <td>0.098166</td>\n",
              "      <td>0.830669</td>\n",
              "      <td>0.904761</td>\n",
              "      <td>(0.8417004437990038, 0.8342557925533596, 0.8306691025440553, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.095300</td>\n",
              "      <td>0.070622</td>\n",
              "      <td>0.859958</td>\n",
              "      <td>0.922631</td>\n",
              "      <td>(0.8695333059147937, 0.8626029449425986, 0.85995836901009, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.070200</td>\n",
              "      <td>0.056393</td>\n",
              "      <td>0.851714</td>\n",
              "      <td>0.917133</td>\n",
              "      <td>(0.8613432447905102, 0.8551060971807736, 0.8517135022078148, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.053900</td>\n",
              "      <td>0.046937</td>\n",
              "      <td>0.850235</td>\n",
              "      <td>0.915995</td>\n",
              "      <td>(0.8597441677808924, 0.8538792431801696, 0.8502348251523739, None)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='701' max='6704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 701/6704 04:42 < 40:24, 2.48 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"(0.8048262400993602, 0.7932012922999004, 0.7902166867298706, None)\" of type <class 'tuple'> for key \"eval/All\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-27000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-27000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-27000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-25500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-27500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-27500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-27500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-27500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-27500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-26000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-28000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-28000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-28000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-26500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-28500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-28500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-28500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-28500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-28500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-27000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-29000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-29000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-29000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-27500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-29500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-29500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-29500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-29500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-29500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-28000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-30000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-30000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-30000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-28500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-30500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-30500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-30500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-30500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-30500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-29000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-31000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-31000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-31000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-29500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-31500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-31500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-31500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-31500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-31500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-30000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-32000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-32000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-32000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-30500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-32500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-32500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-32500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-32500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-32500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-31000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-33000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-33000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-33000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-33000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-33000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-31500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-33500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-33500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-33500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-33500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-33500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-32000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-34000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-34000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-34000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-34000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-34000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-32500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-34500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-34500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-34500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-34500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-34500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-33000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-35000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-35000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-35000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-35000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-35000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-33500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-35500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-35500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-35500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-35500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-35500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-34000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-36000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-36000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-36000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-36000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-36000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-34500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-36500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-36500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-36500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-36500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-36500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-35000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-37000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-37000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-37000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-37000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-37000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-35500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-37500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-37500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-37500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-37500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-37500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-36000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-38000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-38000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-38000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-38000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-38000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-36500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-38500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-38500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-38500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-38500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-38500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-37000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-39000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-39000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-39000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-39000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-39000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-37500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-39500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-39500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-39500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-39500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-39500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-38000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-40000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-40000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-40000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-40000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-40000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-38500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-40500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-40500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-40500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-40500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-40500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-39000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-41000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-41000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-41000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-41000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-41000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-39500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-41500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-41500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-41500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-41500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-41500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-40000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-42000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-42000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-42000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-42000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-42000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-40500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-42500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-42500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-42500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-42500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-42500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-41000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-43000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-43000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-43000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-43000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-43000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-41500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-43500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-43500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-43500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-43500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-43500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-42000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-44000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-44000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-44000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-44000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-44000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-42500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-44500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-44500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-44500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-44500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-44500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-43000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-45000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-45000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-45000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-45000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-45000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-43500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-45500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-45500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-45500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-45500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-45500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-44000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-46000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-46000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-46000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-46000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-46000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-44500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-46500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-46500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-46500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-46500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-46500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-45000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-47000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-47000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-47000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-47000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-47000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-45500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-47500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-47500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-47500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-47500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-47500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-46000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-48000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-48000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-48000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-48000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-48000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-46500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-48500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-48500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-48500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-48500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-48500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-47000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-49000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-49000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-49000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-49000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-49000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-47500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-49500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-49500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-49500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-49500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-49500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-48000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-50000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-50000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-50000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-50000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-50000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-48500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-50500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-50500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-50500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-50500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-50500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-49000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-51000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-51000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-51000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-51000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-51000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-49500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-51500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-51500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-51500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-51500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-51500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-50000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-52000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-52000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-52000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-52000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-52000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-50500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-52500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-52500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-52500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-52500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-52500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-51000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-53000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-53000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-53000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-53000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-53000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-51500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-53500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-53500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-53500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-53500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-53500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-52000] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 429047\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"(0.8417004437990038, 0.8342557925533596, 0.8306691025440553, None)\" of type <class 'tuple'> for key \"eval/All\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-54000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-54000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-54000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-54000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-54000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-52500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-54500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-54500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-54500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-54500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-54500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-53000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-55000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-55000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-55000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-55000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-55000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-53500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-55500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-55500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-55500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-55500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-55500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-54000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-56000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-56000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-56000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-56000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-56000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-54500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-56500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-56500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-56500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-56500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-56500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-55000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-57000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-57000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-57000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-57000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-57000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-55500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-57500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-57500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-57500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-57500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-57500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-56000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-58000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-58000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-58000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-58000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-58000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-56500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-58500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-58500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-58500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-58500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-58500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-57000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-59000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-59000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-59000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-59000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-59000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-57500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-59500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-59500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-59500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-59500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-59500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-58000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-60000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-60000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-60000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-60000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-60000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-58500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-60500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-60500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-60500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-60500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-60500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-59000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-61000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-61000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-61000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-61000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-61000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-59500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-61500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-61500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-61500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-61500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-61500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-60000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-62000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-62000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-62000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-62000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-62000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-60500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-62500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-62500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-62500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-62500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-62500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-61000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-63000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-63000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-63000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-63000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-63000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-61500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-63500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-63500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-63500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-63500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-63500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-62000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-64000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-64000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-64000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-64000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-64000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-62500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-64500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-64500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-64500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-64500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-64500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-63000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-65000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-65000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-65000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-65000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-65000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-63500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-65500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-65500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-65500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-65500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-65500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-64000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-66000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-66000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-66000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-66000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-66000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-64500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-66500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-66500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-66500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-66500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-66500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-65000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-67000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-67000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-67000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-67000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-67000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-65500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-67500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-67500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-67500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-67500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-67500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-66000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-68000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-68000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-68000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-68000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-68000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-66500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-68500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-68500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-68500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-68500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-68500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-67000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-69000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-69000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-69000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-69000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-69000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-67500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-69500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-69500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-69500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-69500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-69500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-68000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-70000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-70000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-70000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-70000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-70000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-68500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-70500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-70500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-70500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-70500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-70500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-69000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-71000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-71000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-71000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-71000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-71000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-69500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-71500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-71500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-71500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-71500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-71500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-70000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-72000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-72000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-72000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-72000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-72000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-70500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-72500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-72500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-72500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-72500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-72500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-71000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-73000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-73000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-73000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-73000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-73000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-71500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-73500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-73500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-73500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-73500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-73500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-72000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-74000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-74000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-74000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-74000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-74000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-72500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-74500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-74500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-74500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-74500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-74500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-73000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-75000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-75000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-75000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-75000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-75000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-73500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-75500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-75500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-75500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-75500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-75500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-74000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-76000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-76000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-76000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-76000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-76000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-74500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-76500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-76500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-76500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-76500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-76500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-75000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-77000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-77000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-77000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-77000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-77000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-75500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-77500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-77500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-77500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-77500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-77500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-76000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-78000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-78000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-78000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-78000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-78000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-76500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-78500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-78500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-78500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-78500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-78500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-77000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-79000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-79000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-79000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-79000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-79000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-77500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-79500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-79500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-79500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-79500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-79500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-78000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-80000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-80000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-80000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-80000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-80000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-78500] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 429047\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"(0.8695333059147937, 0.8626029449425986, 0.85995836901009, None)\" of type <class 'tuple'> for key \"eval/All\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-80500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-80500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-80500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-80500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-80500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-79000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-81000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-81000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-81000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-81000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-81000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-79500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-81500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-81500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-81500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-81500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-81500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-80000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-82000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-82000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-82000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-82000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-82000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-80500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-82500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-82500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-82500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-82500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-82500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-81000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-83000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-83000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-83000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-83000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-83000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-81500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-83500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-83500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-83500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-83500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-83500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-82000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-84000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-84000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-84000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-84000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-84000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-82500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-84500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-84500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-84500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-84500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-84500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-83000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-85000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-85000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-85000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-85000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-85000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-83500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-85500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-85500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-85500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-85500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-85500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-84000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-86000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-86000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-86000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-86000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-86000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-84500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-86500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-86500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-86500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-86500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-86500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-85000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-87000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-87000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-87000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-87000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-87000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-85500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-87500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-87500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-87500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-87500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-87500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-86000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-88000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-88000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-88000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-88000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-88000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-86500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-88500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-88500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-88500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-88500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-88500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-87000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-89000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-89000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-89000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-89000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-89000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-87500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-89500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-89500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-89500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-89500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-89500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-88000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-90000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-90000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-90000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-90000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-90000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-88500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-90500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-90500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-90500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-90500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-90500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-89000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-91000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-91000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-91000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-91000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-91000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-89500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-91500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-91500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-91500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-91500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-91500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-90000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-92000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-92000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-92000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-92000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-92000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-90500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-92500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-92500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-92500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-92500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-92500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-91000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-93000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-93000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-93000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-93000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-93000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-91500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-93500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-93500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-93500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-93500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-93500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-92000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-94000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-94000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-94000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-94000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-94000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-92500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-94500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-94500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-94500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-94500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-94500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-93000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-95000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-95000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-95000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-95000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-95000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-93500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-95500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-95500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-95500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-95500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-95500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-94000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-96000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-96000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-96000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-96000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-96000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-94500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-96500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-96500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-96500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-96500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-96500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-95000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-97000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-97000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-97000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-97000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-97000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-95500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-97500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-97500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-97500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-97500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-97500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-96000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-98000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-98000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-98000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-98000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-98000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-96500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-98500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-98500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-98500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-98500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-98500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-97000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-99000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-99000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-99000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-99000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-99000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-97500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-99500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-99500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-99500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-99500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-99500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-98000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-100000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-100000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-100000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-100000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-100000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-98500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-100500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-100500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-100500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-100500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-100500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-99000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-101000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-101000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-101000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-101000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-101000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-99500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-101500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-101500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-101500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-101500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-101500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-100000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-102000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-102000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-102000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-102000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-102000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-100500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-102500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-102500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-102500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-102500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-102500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-101000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-103000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-103000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-103000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-103000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-103000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-101500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-103500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-103500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-103500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-103500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-103500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-102000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-104000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-104000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-104000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-104000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-104000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-102500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-104500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-104500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-104500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-104500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-104500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-103000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-105000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-105000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-105000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-105000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-105000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-103500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-105500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-105500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-105500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-105500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-105500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-104000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-106000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-106000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-106000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-106000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-106000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-104500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-106500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-106500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-106500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-106500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-106500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-105000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-107000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-107000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-107000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-107000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-107000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-105500] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 429047\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"(0.8613432447905102, 0.8551060971807736, 0.8517135022078148, None)\" of type <class 'tuple'> for key \"eval/All\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-107500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-107500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-107500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-107500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-107500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-106000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-108000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-108000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-108000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-108000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-108000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-106500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-108500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-108500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-108500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-108500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-108500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-107000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-109000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-109000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-109000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-109000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-109000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-107500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-109500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-109500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-109500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-109500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-109500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-108000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-110000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-110000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-110000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-110000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-110000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-108500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-110500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-110500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-110500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-110500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-110500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-109000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-111000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-111000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-111000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-111000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-111000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-109500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-111500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-111500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-111500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-111500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-111500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-110000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-112000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-112000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-112000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-112000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-112000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-110500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-112500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-112500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-112500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-112500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-112500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-111000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-113000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-113000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-113000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-113000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-113000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-111500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-113500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-113500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-113500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-113500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-113500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-112000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-114000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-114000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-114000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-114000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-114000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-112500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-114500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-114500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-114500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-114500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-114500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-113000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-115000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-115000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-115000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-115000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-115000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-113500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-115500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-115500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-115500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-115500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-115500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-114000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-116000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-116000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-116000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-116000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-116000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-114500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-116500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-116500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-116500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-116500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-116500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-115000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-117000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-117000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-117000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-117000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-117000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-115500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-117500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-117500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-117500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-117500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-117500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-116000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-118000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-118000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-118000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-118000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-118000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-116500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-118500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-118500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-118500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-118500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-118500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-117000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-119000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-119000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-119000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-119000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-119000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-117500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-119500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-119500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-119500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-119500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-119500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-118000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-120000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-120000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-120000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-120000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-120000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-118500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-120500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-120500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-120500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-120500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-120500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-119000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-121000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-121000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-121000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-121000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-121000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-119500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-121500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-121500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-121500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-121500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-121500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-120000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-122000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-122000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-122000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-122000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-122000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-120500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-122500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-122500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-122500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-122500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-122500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-121000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-123000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-123000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-123000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-123000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-123000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-121500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-123500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-123500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-123500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-123500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-123500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-122000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-124000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-124000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-124000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-124000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-124000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-122500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-124500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-124500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-124500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-124500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-124500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-123000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-125000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-125000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-125000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-125000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-125000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-123500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-125500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-125500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-125500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-125500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-125500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-124000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-126000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-126000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-126000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-126000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-126000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-124500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-126500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-126500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-126500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-126500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-126500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-125000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-127000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-127000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-127000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-127000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-127000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-125500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-127500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-127500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-127500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-127500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-127500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-126000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-128000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-128000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-128000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-128000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-128000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-126500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-128500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-128500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-128500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-128500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-128500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-127000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-129000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-129000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-129000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-129000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-129000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-127500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-129500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-129500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-129500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-129500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-129500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-128000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-130000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-130000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-130000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-130000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-130000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-128500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-130500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-130500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-130500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-130500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-130500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-129000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-131000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-131000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-131000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-131000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-131000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-129500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-131500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-131500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-131500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-131500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-131500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-130000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-132000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-132000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-132000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-132000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-132000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-130500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-132500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-132500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-132500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-132500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-132500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-131000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-133000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-133000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-133000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-133000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-133000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-131500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-133500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-133500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-133500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-133500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-133500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-132000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-134000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-134000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-134000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-134000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-134000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-132500] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 429047\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"(0.8597441677808924, 0.8538792431801696, 0.8502348251523739, None)\" of type <class 'tuple'> for key \"eval/All\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-134500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-134500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-134500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-134500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-134500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-133000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-135000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-135000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-135000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-135000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-135000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-133500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-135500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-135500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-135500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-135500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-135500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-134000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-136000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-136000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-136000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-136000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-136000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-134500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-136500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-136500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-136500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-136500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-136500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-135000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-137000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-137000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-137000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-137000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-137000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-135500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-137500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-137500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-137500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-137500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-137500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-136000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-138000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-138000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-138000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-138000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-138000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-136500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-138500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-138500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-138500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-138500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-138500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-137000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-139000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-139000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-139000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-139000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-139000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-137500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-139500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-139500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-139500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-139500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-139500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-138000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-140000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-140000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-140000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-140000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-140000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-138500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-140500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-140500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-140500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-140500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-140500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-139000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-141000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-141000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-141000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-141000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-141000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-139500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-141500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-141500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-141500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-141500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-141500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-140000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-142000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-142000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-142000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-142000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-142000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-140500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-142500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-142500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-142500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-142500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-142500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-141000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-143000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-143000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-143000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-143000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-143000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-141500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-143500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-143500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-143500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-143500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-143500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-142000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-144000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-144000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-144000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-144000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-144000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-142500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-144500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-144500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-144500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-144500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-144500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-143000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-145000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-145000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-145000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-145000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-145000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-143500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-145500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-145500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-145500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-145500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-145500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-144000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-146000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-146000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-146000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-146000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-146000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-144500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-146500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-146500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-146500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-146500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-146500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-145000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-147000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-147000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-147000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-147000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-147000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-145500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-147500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-147500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-147500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-147500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-147500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-146000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-148000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-148000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-148000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-148000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-148000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-146500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-148500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-148500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-148500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-148500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-148500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-147000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-149000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-149000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-149000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-149000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-149000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-147500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-149500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-149500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-149500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-149500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-149500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-148000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-150000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-150000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-150000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-150000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-150000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-148500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-150500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-150500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-150500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-150500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-150500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-149000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-151000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-151000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-151000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-151000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-151000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-149500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-151500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-151500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-151500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-151500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-151500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-150000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-152000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-152000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-152000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-152000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-152000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-150500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-152500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-152500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-152500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-152500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-152500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-151000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-153000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-153000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-153000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-153000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-153000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-151500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-153500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-153500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-153500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-153500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-153500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-152000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-154000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-154000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-154000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-154000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-154000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-152500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-154500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-154500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-154500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-154500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-154500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-153000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-155000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-155000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-155000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-155000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-155000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-153500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-155500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-155500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-155500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-155500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-155500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-154000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-156000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-156000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-156000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-156000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-156000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-154500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-156500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-156500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-156500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-156500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-156500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-155000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-157000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-157000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-157000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-157000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-157000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-155500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-157500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-157500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-157500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-157500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-157500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-156000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-158000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-158000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-158000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-158000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-158000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-156500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-158500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-158500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-158500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-158500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-158500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-157000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-159000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-159000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-159000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-159000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-159000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-157500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-159500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-159500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-159500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-159500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-159500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-158000] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-160000\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-160000/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-160000/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-160000/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-160000/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-158500] due to args.save_total_limit\n",
            "Saving model checkpoint to AraBART-finetuned-word-to-norm/checkpoint-160500\n",
            "Configuration saved in AraBART-finetuned-word-to-norm/checkpoint-160500/config.json\n",
            "Model weights saved in AraBART-finetuned-word-to-norm/checkpoint-160500/pytorch_model.bin\n",
            "tokenizer config file saved in AraBART-finetuned-word-to-norm/checkpoint-160500/tokenizer_config.json\n",
            "Special tokens file saved in AraBART-finetuned-word-to-norm/checkpoint-160500/special_tokens_map.json\n",
            "Deleting older checkpoint [AraBART-finetuned-word-to-norm/checkpoint-159000] due to args.save_total_limit\n",
            "The following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Norm, index, word. If Norm, index, word are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 429047\n",
            "  Batch size = 64\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkOmGiKiY_pJ"
      },
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "AOXly447jZhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ],
      "metadata": {
        "id": "1sqNX924HAJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ],
      "metadata": {
        "id": "mJQGSUGRHXHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.predictions"
      ],
      "metadata": {
        "id": "1wGYI3NHH2QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.label_ids"
      ],
      "metadata": {
        "id": "gPtg0x5tHxqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
        "\n",
        "# Replace -100 in the labels as we can't decode them.\n",
        "labels = np.where(predictions.label_ids != -100, predictions.label_ids, tokenizer.pad_token_id)\n",
        "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "# Some simple post-processing\n",
        "decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)"
      ],
      "metadata": {
        "id": "aL-HTTXTIL4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "f1_score(decoded_labels, decoded_preds, average='macro')\n",
        "accuracy_score(decoded_labels, decoded_preds)"
      ],
      "metadata": {
        "id": "3boqWqnaHTYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "SPIRAL using AraBart.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f924eca49b548dbbc407f34d141eb1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1123bcbe1a947a3a1859dc9091a602b",
              "IPY_MODEL_0ca046404858491b963f18b1412e406e",
              "IPY_MODEL_815f815399c34262ad5fd8a7fa00fca9"
            ],
            "layout": "IPY_MODEL_0fbd859033104837929e81833c1d841b"
          }
        },
        "b1123bcbe1a947a3a1859dc9091a602b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0213cad7078f484999367aa60ecb43f9",
            "placeholder": "​",
            "style": "IPY_MODEL_5a26fca2d84743e39577d67350e6fea8",
            "value": "Downloading data files: 100%"
          }
        },
        "0ca046404858491b963f18b1412e406e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85b235646e7a4b2a816b95f437b1e712",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24c7c776f9154502afd6b3f4ce164613",
            "value": 2
          }
        },
        "815f815399c34262ad5fd8a7fa00fca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f39196abbfa8479d9aacedc287b7caea",
            "placeholder": "​",
            "style": "IPY_MODEL_9becee0627ee4d7b8199512fc2d0a20a",
            "value": " 2/2 [00:00&lt;00:00, 73.39it/s]"
          }
        },
        "0fbd859033104837929e81833c1d841b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0213cad7078f484999367aa60ecb43f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a26fca2d84743e39577d67350e6fea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85b235646e7a4b2a816b95f437b1e712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c7c776f9154502afd6b3f4ce164613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f39196abbfa8479d9aacedc287b7caea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9becee0627ee4d7b8199512fc2d0a20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c91bfc5064e452d962e90dbad12c18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a78a840ca934459bdc14d491d5aec20",
              "IPY_MODEL_ce0c226bb8244176a78cd98cfea6afe2",
              "IPY_MODEL_2caef253d9b0433290da2bdcce24db4e"
            ],
            "layout": "IPY_MODEL_0537aca5b6574a359d6a5e6583080b8d"
          }
        },
        "5a78a840ca934459bdc14d491d5aec20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38dc8aa992444841be12459b86ee70dd",
            "placeholder": "​",
            "style": "IPY_MODEL_433fb79895e6455994154f08d0f923d7",
            "value": "Extracting data files: 100%"
          }
        },
        "ce0c226bb8244176a78cd98cfea6afe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5cdbf61fa324d87a09bd48919586025",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_769376e3127442959351b6dcf86e3d21",
            "value": 2
          }
        },
        "2caef253d9b0433290da2bdcce24db4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2364e7ac3b47759892f33e54964de8",
            "placeholder": "​",
            "style": "IPY_MODEL_aab4504aa5174624bfd6afd43cb55405",
            "value": " 2/2 [00:00&lt;00:00, 32.71it/s]"
          }
        },
        "0537aca5b6574a359d6a5e6583080b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38dc8aa992444841be12459b86ee70dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433fb79895e6455994154f08d0f923d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5cdbf61fa324d87a09bd48919586025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "769376e3127442959351b6dcf86e3d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da2364e7ac3b47759892f33e54964de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab4504aa5174624bfd6afd43cb55405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce541cbf6b946e2b25a54e2438257a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c285a66e444d38911e15c5bc4ecef0",
              "IPY_MODEL_b78f8a9e7b65449495d56a592e5a19e6",
              "IPY_MODEL_cbe3186924d24e989c1d8a3c91e9ee47"
            ],
            "layout": "IPY_MODEL_cb7e2a953b624620afac3b82c03c2d12"
          }
        },
        "e0c285a66e444d38911e15c5bc4ecef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcf6976eff6b49afbd8c108819b83173",
            "placeholder": "​",
            "style": "IPY_MODEL_c1b8ae7b88c74633b83b246bd9cfcf17",
            "value": ""
          }
        },
        "b78f8a9e7b65449495d56a592e5a19e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c40e99ace384424db5aa43d9642acc5a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caaba9446a2d474b93e5de66d564cbba",
            "value": 1
          }
        },
        "cbe3186924d24e989c1d8a3c91e9ee47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3318ffa4e53c4a218bd0fa9b85d50614",
            "placeholder": "​",
            "style": "IPY_MODEL_1f7de103b1084e52a141df35c5beae50",
            "value": " 170/? [00:03&lt;00:00, 49.84 tables/s]"
          }
        },
        "cb7e2a953b624620afac3b82c03c2d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf6976eff6b49afbd8c108819b83173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1b8ae7b88c74633b83b246bd9cfcf17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c40e99ace384424db5aa43d9642acc5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "caaba9446a2d474b93e5de66d564cbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3318ffa4e53c4a218bd0fa9b85d50614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7de103b1084e52a141df35c5beae50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6953a6dc78e2430eba14e9bbf05b0c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5815ce8fb8544a3a97b9e93b1876a199",
              "IPY_MODEL_6ada4f3ec9bc4278b4e8e0ce221097c1",
              "IPY_MODEL_54888541df524bc68baf6f38dd60b8cf"
            ],
            "layout": "IPY_MODEL_a42e80ee6dca4672b1b3f120508bc5b4"
          }
        },
        "5815ce8fb8544a3a97b9e93b1876a199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05d027ee85594c22b70ab132cfd403b0",
            "placeholder": "​",
            "style": "IPY_MODEL_bf0bdcd1e3d94576b2bb4504e314b86b",
            "value": ""
          }
        },
        "6ada4f3ec9bc4278b4e8e0ce221097c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45dd3d60631e48bc9f6548e9ca42728f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f988ca6e4ae54b07a1203c8e8bb06abc",
            "value": 1
          }
        },
        "54888541df524bc68baf6f38dd60b8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bf7c4085f4d46a3aac22ef99f90d184",
            "placeholder": "​",
            "style": "IPY_MODEL_b8d7fbc83cf9494291fe08df06bcb6f4",
            "value": " 38/? [00:00&lt;00:00, 52.09 tables/s]"
          }
        },
        "a42e80ee6dca4672b1b3f120508bc5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d027ee85594c22b70ab132cfd403b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf0bdcd1e3d94576b2bb4504e314b86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45dd3d60631e48bc9f6548e9ca42728f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f988ca6e4ae54b07a1203c8e8bb06abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bf7c4085f4d46a3aac22ef99f90d184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d7fbc83cf9494291fe08df06bcb6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3f3c0629e724047b5982f82d9bb6ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4a990cc7035495d9893cb4fe0f8b186",
              "IPY_MODEL_a1cd1d6f81f94a529eedead7552facb1",
              "IPY_MODEL_798128f807c54452b1b0e6857c7f8faf"
            ],
            "layout": "IPY_MODEL_f4419c64d9ef42108031815e2fbc9b65"
          }
        },
        "d4a990cc7035495d9893cb4fe0f8b186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7d5ab2c00c54b1d982b63245ddc47de",
            "placeholder": "​",
            "style": "IPY_MODEL_45be3ec808a74021b7c7df26189a4fac",
            "value": "100%"
          }
        },
        "a1cd1d6f81f94a529eedead7552facb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e95d491ce94b4da5fbb24f8e739b39",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cf5bef130ad4fc4b66c525d48dc9ca6",
            "value": 2
          }
        },
        "798128f807c54452b1b0e6857c7f8faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_516c8eb658a649a28180dc187d24b1a2",
            "placeholder": "​",
            "style": "IPY_MODEL_369fc48afcbb416587377793ddaddf9e",
            "value": " 2/2 [00:00&lt;00:00, 55.22it/s]"
          }
        },
        "f4419c64d9ef42108031815e2fbc9b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7d5ab2c00c54b1d982b63245ddc47de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45be3ec808a74021b7c7df26189a4fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04e95d491ce94b4da5fbb24f8e739b39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf5bef130ad4fc4b66c525d48dc9ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "516c8eb658a649a28180dc187d24b1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "369fc48afcbb416587377793ddaddf9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44a9b2ffb744407587e49a795646c3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6d6719b993d4debbbb5537e8c2b252e",
              "IPY_MODEL_a7acf550fb03470d8679a473d037ac2c",
              "IPY_MODEL_2beef0a8b8954d329ee88ae737a4d7bc"
            ],
            "layout": "IPY_MODEL_3cddd5e2b8f34993a77f42f6be81f220"
          }
        },
        "a6d6719b993d4debbbb5537e8c2b252e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b2d54608574c1db8a70e0668f88e50",
            "placeholder": "​",
            "style": "IPY_MODEL_7da4f8dc151b467a84d67d13d60b84a9",
            "value": "Downloading builder script: "
          }
        },
        "a7acf550fb03470d8679a473d037ac2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eeff5dd26b44482bb714ebe7128f998",
            "max": 2848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a29342bcdf3d46969085fafb7f3446a6",
            "value": 2848
          }
        },
        "2beef0a8b8954d329ee88ae737a4d7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ef2d19f8cd4139a2359f81ba80e8cc",
            "placeholder": "​",
            "style": "IPY_MODEL_c4c606adbd3643399db2aa13abb40a63",
            "value": " 7.65k/? [00:00&lt;00:00, 260kB/s]"
          }
        },
        "3cddd5e2b8f34993a77f42f6be81f220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b2d54608574c1db8a70e0668f88e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7da4f8dc151b467a84d67d13d60b84a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eeff5dd26b44482bb714ebe7128f998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a29342bcdf3d46969085fafb7f3446a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99ef2d19f8cd4139a2359f81ba80e8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4c606adbd3643399db2aa13abb40a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f367d2a50d34312b1ce023f10b6904e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09cd0bbb17a7469caf70fce46f1c0fee",
              "IPY_MODEL_e028932bd7424cffb3c00a46ddfb0299",
              "IPY_MODEL_a9a04715d0db4a29a2b9223e9f473928"
            ],
            "layout": "IPY_MODEL_7d3415a72e4144e9a6fdcfcaf4fca5d4"
          }
        },
        "09cd0bbb17a7469caf70fce46f1c0fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_636b7dd8d8b249abb581b16543e35bba",
            "placeholder": "​",
            "style": "IPY_MODEL_e9f7c65f206f4c9d975ba68b48543865",
            "value": "Downloading: 100%"
          }
        },
        "e028932bd7424cffb3c00a46ddfb0299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed05134573c0443494aafd5b21c69d28",
            "max": 1392,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9abf7b92b623460c949796d207527472",
            "value": 1392
          }
        },
        "a9a04715d0db4a29a2b9223e9f473928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a1b39ca9224e37a021777ee0cee5ef",
            "placeholder": "​",
            "style": "IPY_MODEL_d6b93a0743334897acd0efe98c50532e",
            "value": " 1.36k/1.36k [00:00&lt;00:00, 40.3kB/s]"
          }
        },
        "7d3415a72e4144e9a6fdcfcaf4fca5d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636b7dd8d8b249abb581b16543e35bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9f7c65f206f4c9d975ba68b48543865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed05134573c0443494aafd5b21c69d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9abf7b92b623460c949796d207527472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03a1b39ca9224e37a021777ee0cee5ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6b93a0743334897acd0efe98c50532e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ec1397013594652b81962d443518503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61106adbd75b41e0a789944c63e9be26",
              "IPY_MODEL_f16e37a798c34adcaedc515187c085d0",
              "IPY_MODEL_093932d6083f415787f68061ec1b4530"
            ],
            "layout": "IPY_MODEL_9dff117a40244d98b4b2e58a6cee3fea"
          }
        },
        "61106adbd75b41e0a789944c63e9be26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ac40a0ea18c429982bde6e560413669",
            "placeholder": "​",
            "style": "IPY_MODEL_20ed39460100414ba2c215004530a535",
            "value": "Downloading: 100%"
          }
        },
        "f16e37a798c34adcaedc515187c085d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b54d5812734831aa855ea2db7daa29",
            "max": 1315170,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2de39ce7144c4b21babe492379ec9d25",
            "value": 1315170
          }
        },
        "093932d6083f415787f68061ec1b4530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_253e1d153ad547118dd141d9b606dd39",
            "placeholder": "​",
            "style": "IPY_MODEL_b265bdce69d94b46afbd32f13929c9e6",
            "value": " 1.25M/1.25M [00:01&lt;00:00, 1.60MB/s]"
          }
        },
        "9dff117a40244d98b4b2e58a6cee3fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac40a0ea18c429982bde6e560413669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ed39460100414ba2c215004530a535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21b54d5812734831aa855ea2db7daa29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2de39ce7144c4b21babe492379ec9d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "253e1d153ad547118dd141d9b606dd39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b265bdce69d94b46afbd32f13929c9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc4bca72ffab4b2d80fa132ab4045e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b941cc09c7e41e3ac5e4affcabd85e5",
              "IPY_MODEL_3df67841aeab49b7933b6a03b4100707",
              "IPY_MODEL_031b2eb51cf14e0eba3d1c2522b2bd8c"
            ],
            "layout": "IPY_MODEL_f374a6338af949a488e66a3acfee74b9"
          }
        },
        "1b941cc09c7e41e3ac5e4affcabd85e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4418b47dff764c67a3efbb8bb2b95f18",
            "placeholder": "​",
            "style": "IPY_MODEL_db38d17ee9a94aa5ab645470a88de629",
            "value": "100%"
          }
        },
        "3df67841aeab49b7933b6a03b4100707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adbfdf5725f84e9190d64090d60d7cd0",
            "max": 1717,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7c18e78c6914d65b30a8eba8256c0d6",
            "value": 1717
          }
        },
        "031b2eb51cf14e0eba3d1c2522b2bd8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47c343c2abcc47e5b23cffa03bda56d4",
            "placeholder": "​",
            "style": "IPY_MODEL_37742003dab84ecfaf22cac2e0e3fdba",
            "value": " 1717/1717 [00:56&lt;00:00, 30.69ba/s]"
          }
        },
        "f374a6338af949a488e66a3acfee74b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4418b47dff764c67a3efbb8bb2b95f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db38d17ee9a94aa5ab645470a88de629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adbfdf5725f84e9190d64090d60d7cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c18e78c6914d65b30a8eba8256c0d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47c343c2abcc47e5b23cffa03bda56d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37742003dab84ecfaf22cac2e0e3fdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae60d8e07c444ffeb174a25e8b1bbdb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a5e9e9966734468883c4f356fa7fb82",
              "IPY_MODEL_976dbbe7783a4bcdb7d6b44921b776d7",
              "IPY_MODEL_94c4dc2ba5904abb90fee1a9de8138e4"
            ],
            "layout": "IPY_MODEL_af6942188b3047d28845b13d8c5979af"
          }
        },
        "2a5e9e9966734468883c4f356fa7fb82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf63cd35576b49a19e1ec557c0fe8b2d",
            "placeholder": "​",
            "style": "IPY_MODEL_0b321c55ba90496ab1d0d69efc3de969",
            "value": "100%"
          }
        },
        "976dbbe7783a4bcdb7d6b44921b776d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18cc64d6def74040b473fe790354d7a8",
            "max": 430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd198a4066e64333a4505b9691ddd4d6",
            "value": 430
          }
        },
        "94c4dc2ba5904abb90fee1a9de8138e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b019a6d034749188af374b63666e650",
            "placeholder": "​",
            "style": "IPY_MODEL_bd66d23dacdb4c6ca595e2858524ea6e",
            "value": " 430/430 [00:14&lt;00:00, 28.43ba/s]"
          }
        },
        "af6942188b3047d28845b13d8c5979af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf63cd35576b49a19e1ec557c0fe8b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b321c55ba90496ab1d0d69efc3de969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18cc64d6def74040b473fe790354d7a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd198a4066e64333a4505b9691ddd4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b019a6d034749188af374b63666e650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd66d23dacdb4c6ca595e2858524ea6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f2f04c1009c497c840566bc1bda0f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8f889d1eef24019ae7f2e2811e3f139",
              "IPY_MODEL_99a7bc0f0fba4be9a1e3192c7f90ff82",
              "IPY_MODEL_634d5bb4cfd64ba59d4b02b81e6e6114"
            ],
            "layout": "IPY_MODEL_e248f3c7ac9941be91c748f517858d4d"
          }
        },
        "e8f889d1eef24019ae7f2e2811e3f139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5883db857d0c46ffbbc652c4d4797475",
            "placeholder": "​",
            "style": "IPY_MODEL_af245c9882ef47408748109a5888fcb7",
            "value": "Downloading: 100%"
          }
        },
        "99a7bc0f0fba4be9a1e3192c7f90ff82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e44d038eb3846b39322b95d33405a40",
            "max": 556983662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6554bfce57244512acfeb9b2c7f5d0c8",
            "value": 556983662
          }
        },
        "634d5bb4cfd64ba59d4b02b81e6e6114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2995a1b87f6942bd87889fb64b946996",
            "placeholder": "​",
            "style": "IPY_MODEL_744dd9c6ca4443ebbc66d6256795bd78",
            "value": " 531M/531M [00:31&lt;00:00, 18.7MB/s]"
          }
        },
        "e248f3c7ac9941be91c748f517858d4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5883db857d0c46ffbbc652c4d4797475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af245c9882ef47408748109a5888fcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e44d038eb3846b39322b95d33405a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6554bfce57244512acfeb9b2c7f5d0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2995a1b87f6942bd87889fb64b946996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "744dd9c6ca4443ebbc66d6256795bd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}